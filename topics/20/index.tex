\documentclass[numbers=endperiod, bibliography=totocnumbered]{scrartcl}

\usepackage{../../common/common_packages}
\usepackage{../../common/macros}
\usepackage{../../common/theorem_styles}

% Bibliography
\addbibresource{./references.bib}

% Document
\title{Тема 20}
\subtitle{Точкови и интервални оценки за параметрите на нормалното разпределение.}
\author{Янис Василев, \Email{ianis@ivasilev.net}}
\date{4 юли 2019}

\begin{document}

\maketitle

\section{Теория}

Общата теория е базирана на~\cite{DimitrovYanev}, а оценките за нормалното разпределение - на~\cite{ASLectures}.

\subsection{Анотация}

Изложената анотацията е взета от конспекта~\cite{Syllabus} за 2018г.

\begin{enumerate}
  \item Определения за точкови и интервални оценки
  \item Свойства на точковите оценки
  \item Неравенство на Рао-Крамер
  \item Доверителни интервали за параметрите на нормалното разпределение
\end{enumerate}

\subsection{Основни понятия}

Считаме, че е зададено вероятностно пространство \( (\Omega, \F, \Prob) \).

\begin{definition}[Извадки]
  Нека \( \xi \) е случайна величина над \( (\Omega, \F, \Prob) \). Множеството от елементарни събития \( \Omega \) в статистиката често се нарича \underLine{генерална съвкупност}.

  \begin{itemize}
    \item Ако случайните величини \( \xi_1, \ldots, \xi_n \) са независими две по две и имат същото разпределение като \( \xi \), казваме, че \( \xi_1, \ldots, \xi_n \) са \underLine{наблюдения над \( \xi \)} и че те са \underLine{проста извадка с обем \( n \)} над генералната съвкупност \( \Omega \). Понякога ги разглеждаме и като случаен вектор \( \V{\xi_n} = (\xi_1, \ldots, \xi_n) \).
    \item \underLine{Функция на правдоподобие \( l_\xi(x) \)} на случайната величина \( \xi \) наричаме, в абсолютно непрекъснатия случай, плътността на \( \xi \) или, в дискретния случай, функцията на вероятностите на \( \xi \).
    \item \underLine{Функция на правдоподобие \( l(x_1, \ldots, x_n) \) на извадката \( \xi_1, \ldots, \xi_n \)} наричаме функцията на правдоподобие на случайния вектор \( \V{\xi_n} \). При извадки от независими случайни величини, функцията на правдоподобие на извадката е произведение на индивидуалните функции на правдоподобие.
    \item \underLine{Извадково} пространство, съответстващо на извадката \( \xi_1, \ldots, \xi_n \), наричаме множеството \( \SampleSpace \subseteq \R^n \) от стойности на случайния вектор \( \V{\xi_n} \).
    \item \underLine{Реализации} на извадката наричаме вектори от \( \SampleSpace \). Те моделират истинските данни в математическата статистика, съпоставяйки ги на \enquote{теоретичната} извадка \( \xi_1, \ldots, \xi_n \).
  \end{itemize}
\end{definition}

\begin{definition}[Оценки]
  Нека \( \xi_1, \ldots \xi_n \) е проста извадка над случайната величина \( \xi \), чието разпределение не ни е известно. Целта ни е на базата на тази извадка да оценим някакви числени характеристики \( \theta = (\theta_1, \ldots, \theta_m) \) на \( \xi \), които напълно да определят разпределението на \( \xi \). Обикновено \( \theta \) е вектор от моменти на \( \xi \) или, в параметричната статистика, \( \theta \) е някой от параметрите на класът от разпределения, на който се предполага, че принадлежи \( \xi \). Условна вероятност при условие, че \( \theta = \alpha \) е конкретна стойност на вектора, бележим с
  \begin{align*}
    \Prob(\cdot \mid \theta = \alpha)
    &&\text{ или}&&
    \Prob(\cdot \mid \alpha).
  \end{align*}

  Множеството от всички възможни стойности на \( \theta \) ще бележим с \( \Theta \).

  \begin{itemize}
    \item \underLine{Статистика} наричаме всяка измерима функция на извадката. По определение статистиките са случайни величини.

    \item Ако \( \theta = (\theta_1, \ldots, \theta_m) \) са някакви числени характеристика на \( \xi \) и статистиката \( t_n = t_n(x_1, \ldots, x_n) \) не зависи от \( \theta \), казваме, че \( t_n \) е \underLine{точкова оценка за \( \theta \)}.

    \item Стойността \( t_n - \Expect(t_n \mid \theta) \) наричаме \underLine{случайна грешка}, а стойността \( \Expect(t_n \mid \theta) - \theta \) наричаме \underLine{систематична грешка} или \underLine{изместване} на оценката. Разликата
    \begin{align*}
      t_n - \theta = t_n - \Expect(t_n \mid \theta) + \Expect(t_n \mid \theta) - \theta
    \end{align*}
    се разпада на систематична и случайна грешка.

    \item Точковата оценка \( t_n \) за \( \theta \) наричаме \underLine{неизместена}, ако \( \Expect(t_n \mid \theta) = \theta \), и асимптотично неизместена, ако \( \Expect(t_n \mid \theta) \Conv[n \to \infty]{} \theta \).

    \item Точковата оценка \( t_n \) за \( \theta \) наричаме \underLine{състоятелна}, ако \( t_n \Conv[n \to \infty]{} \theta \) по вероятност, т.е.
    \begin{align*}
      \Prob(\Abs{t_n - \theta} > \varepsilon \mid \theta) \Conv[n \to \infty]{} 0~\forall \varepsilon > 0.
    \end{align*}

    Оценката наричаме~\underLine{силно състоятелна}, ако сходимостта е почти сигурна, т.е.
    \begin{align*}
      \Prob(\sup_{k \geq n} \Abs{t_k - \theta} > \varepsilon \mid \theta) \Conv[n \to \infty]{} 0~\forall \varepsilon > 0.
    \end{align*}

    \item Точковата оценка \( t_n \) за \( \theta \) с изместване \( \Expect(t_n) - \theta \) има \underLine{минимална дисперсия}, ако \( \Var(t_n) \leq \Var(u_n) \) за произволна друга точкова оценка \( u_n \) със същото изместване.

    \item Ако векторът \( \theta \) е едномерен (т.е. \( \theta \) е число), \underLine{интервална оценка} с ниво на доверие \( \gamma \) за \( \theta \) наричаме двойка точкови оценки \( a_n \) и \( b_n \) за \( \theta \), за които
    \begin{align*}
      \Prob(a_n \leq \theta \leq b_n \mid \theta) = \gamma
    \end{align*}
    независимо от стойността на \( \theta \).
  \end{itemize}
\end{definition}

\begin{proposition}\label{thm:unbiased-mean}
  За произволна случайна величина \( \xi \) с крайно очакване средноаритметичното
  \begin{align*}
    m_n(x_1, \ldots, x_n) \coloneqq \frac 1 n \sum_{k=1}^n x_k
  \end{align*}
  е неизместена оценка за \( \Expect(\xi) \).
\end{proposition}
\begin{proof}
  Следва директно от линейността на очакването.
\end{proof}

\begin{proposition}\label{thm:unbiased-variation}
  За произволна случайна величина \( \xi \) с крайна дисперсия оценката
  \begin{align*}
    s_n^2(x_1, \ldots, x_n) \coloneqq \frac 1 {n-1} \sum_{k=1}^n {(x_k - \hat \xi(x_1, \ldots, x_n))}^2
  \end{align*}
  е неизместена оценка за \( \Var(\xi) \).
\end{proposition}
\begin{proof}
  Разписваме \( s_n^2 \)
  \begingroup
  \allowdisplaybreaks
  \begin{align*}
    s_n^2(\xi_1, \ldots, \xi_n)
    &=
    \frac 1 {n-1} \sum_{i=1}^n {(\xi_i - \hat \xi(\xi_1, \ldots, \xi_n))}^2
    = \\ &=
    \frac 1 {n-1} \sum_{i=1}^n \left[\xi_i^2 - \frac 2 n \sum_{j=1}^n \xi_i \xi_j + \frac 1 {n^2} {\left( \sum_{j=1}^n \xi_m \right)}^2 \right]
    = \\ &=
    \frac 1 {n-1} \sum_{i=1}^n \left[\xi_i^2 - \frac 2 n \sum_{j=1}^n \xi_i \xi_j + \frac 1 {n^2} \sum_{j=1}^n \sum_{k=1}^n \xi_j \xi_k \right]
    = \\ &=
    \frac 1 {n-1} \sum_{i=1}^n \xi_i^2 - \frac 2 {n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \xi_i \xi_j + \frac n {n^2 (n-1)} \sum_{j=1}^n \sum_{k=1}^n \xi_j \xi_k
    = \\ &=
    \frac 1 {n-1} \sum_{i=1}^n \xi_i^2 - \frac 1 {n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \xi_i \xi_j
    = \\ &=
    \left(\frac 1 {n-1} - \frac 1 {n(n-1)} \right) \sum_{i=1}^n \xi_i^2 - \frac 1 {n(n-1)} \sum_{i=1}^n \sum_{\substack{j=1 \\ j\neq i}}^n \xi_i \xi_j
    = \\ &=
    \frac 1 n \sum_{i=1}^n \xi_i^2 - \frac 1 {n(n-1)} \sum_{i=1}^n \sum_{\substack{j=1 \\ j\neq i}}^n \xi_i \xi_j.
  \end{align*}
  \endgroup

  Като вземем очакване, получаваме
  \begin{align*}
    \Expect(s_n^2 \mid \theta)
    =
    \frac 1 n \sum_{i=1}^n \Expect(\xi_i^2 \mid \theta) - \frac 1 {n(n-1)} \sum_{i=1}^n \sum_{\substack{j=1 \\ j\neq i}}^n \Expect(\xi_i \xi_j \mid \theta)
    = \\ =
    \frac 1 n \sum_{i=1}^n \Expect(\xi_i^2 \mid \theta) - \frac 1 {n(n-1)} \sum_{i=1}^n \sum_{\substack{j=1 \\ j\neq i}}^n \Expect(\xi_i \mid \theta) \Expect(\xi_j \mid \theta)
    = \\ =
    \frac 1 n \sum_{i=1}^n \Expect(\xi^2 \mid \theta) - \frac 1 {n(n-1)} \sum_{i=1}^n \sum_{\substack{j=1 \\ j\neq i}}^n {\Expect(\xi \mid \theta)}^2
    = \\ =
    \frac n n \Expect(\xi^2 \mid \theta) - \frac {n(n-1)} {n(n-1)} {\Expect(\xi \mid \theta)}^2
    =
    \Expect(\xi^2 \mid \theta) - {\Expect(\xi \mid \theta)}^2
    =
    \Var(\xi^2 \mid \theta).
  \end{align*}
\end{proof}

\subsection{Информация на Фишър}

\begin{definition}\label{def:fisher-information}
  Нека \( \theta \in \Theta \) е числена характеристика на \( \xi \), т.е. \( \theta \) е едномерен вектор с възможни стойности в \( \Theta \).

  Разглеждаме \underLine{логаритмичната функция на правдоподобие} \( \ln l_\xi(x \mid \theta) \). За нея ще изискаме допълнителни условия за регулярност:
  \begin{enumerate}
    \item Множеството \( \{ x \in \R \mid l_\xi(x \mid \theta) = 0 \} \), в което не е дефинирана \( \ln l_\xi(x \mid \theta) \), има вероятност \( 0 \) независимо от \( \theta \). Така очакването на \( \ln l_\xi(x \mid \theta) \) е дефинирано почти навсякъде.
    \item Функцията \( \ln l_\xi(x \mid \theta) \) е диференцируема по \( \theta \) за (почти) всяко \( x \) от дефиниционната си област.
    \item За произволно \( \theta \in \Theta \) са изпълнени условията за диференциране под очакването
    \begin{align*}
      \frac {\partial \Expect(\ln l_\xi(x \mid \theta) \mid \theta)} {\partial \theta}
      =
      \Expect\left( \frac {\partial \ln l_\xi(x \mid \theta) \mid \theta)} {\partial \theta} \right).
    \end{align*}
  \end{enumerate}

  \underLine{Информация на Фишър за \( \theta \) на \( \xi \)} наричаме очакването
  \begin{align*}
    \FisherInfo_\xi(\theta) \coloneqq \Var\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right).
  \end{align*}

  Естествено, \( \FisherInfo_\xi(\theta) \) може и да не съществува.

  \underLine{Информация на Фишър за \( \theta \) на извадката \( \xi_1, \ldots, \xi_n \) над \( \xi \)} наричаме информация на Фишър за \( \theta \) на случайния вектор \( (\xi_1, \ldots, \xi_n) \). За прости извадки от независими случайни величини имаме
  \begin{align*}
    \FisherInfo_{(\xi_1, \ldots, \xi_n)}(\theta) = n \FisherInfo_\xi(\theta).
  \end{align*}
\end{definition}

\begin{note}
  Третото условие за регулярност не е необходимо за определението на информация на Фишър, но е необходимо за доказателство на свойствата ѝ.
\end{note}

\begin{theorem}
  \mbox{}
  \begin{enumerate}
    \item Ако информацията \( \FisherInfo_\xi(\theta) \) съществува, тогава
    \begin{align*}
      \FisherInfo_\xi(\theta) = \Expect\left( {\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right)}^2 \right).
    \end{align*}

    \item Ако освен това логаритмичното правдоподобие \( \ln l_\xi (x \mid \theta) \) е двукратно диференцируемо по \( \theta \) и \( \ln l_\xi \) позволява двукратно диференциране под очакването, имаме
    \begin{align*}
      \FisherInfo_\xi(\theta) = -\Expect\left( \frac {\partial^2 \ln l_\xi(\xi \mid \theta)} {\partial \theta^2} \mid \theta \right).
    \end{align*}
  \end{enumerate}
\end{theorem}
\begin{proof}
  Ще докажем теоремата само за абсолютно непрекъснати разпределения. В общия случай римановите интеграли могат да се заменят с интеграли по вероятностни мерки, съответстващи на различните параметри \( \theta \in \Theta \).
  \begin{enumerate}
    \item Разглеждаме очакването
    \begin{align*}
      \Expect\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right)
      =
      \int_{\R} \frac {\partial \ln l_\xi(x \mid \theta)} {\partial \theta} l_\xi(x \mid \theta) dx
      =
      \int_{\R} \frac {\partial l_\xi(x \mid \theta)} {\partial \theta} \frac {l_\xi(x \mid \theta)} {l_\xi(x \mid \theta)} dx
      = \\ =
      \frac \partial {\partial \theta} \int_{\R} l_\xi(x \mid \theta) dx
      =
      \frac {\partial 1} {\partial \theta}
      =
      0.
    \end{align*}

    Тогава
    \begin{align*}
      \FisherInfo_\xi(\theta)
      &=
      \Var\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right)
      = \\ &=
      \Expect\left( {\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right)}^2 \right) + {\left(\Expect\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right)\right)}^2
      = \\ &=
      \Expect\left( {\left( \frac {\partial \ln l_\xi(\xi \mid \theta)} {\partial \theta} \mid \theta \right)}^2 \right).
    \end{align*}

    \item Директно пресмятаме
    \begin{align*}
      &~~~~\Expect\left( \frac {\partial^2 \ln l_\xi(\xi \mid \theta)} {\partial \theta^2} \mid \theta \right)
      = \\ &=
      \int_{\R} \frac {\partial^2 \ln l_\xi(x \mid \theta)} {\partial \theta^2} l_\xi(x \mid \theta) dx
      = \\ &=
      \int_{\R} \frac \partial {\partial \theta} \left(\frac 1 {l_\xi(x \mid \theta)} \cdot \frac {\partial l_\xi(x \mid \theta)} {\partial \theta} \right) \cdot l_\xi(x \mid \theta) dx
      = \\ &=
      \int_{\R} \frac 1 {{(l_\xi(x \mid \theta))}^2} \cdot \left( \frac {\partial^2 l_\xi(x \mid \theta)} {\partial \theta^2} \cdot l_\xi(x \mid \theta) - {\left(\frac {\partial l_\xi(x \mid \theta)} {\partial \theta} \right)}^2 \right) \cdot l_\xi(x \mid \theta) dx
      = \\ &=
      \int_{\R} \frac {\partial^2 l_\xi(x \mid \theta)} {\partial \theta^2} dx - \int_{\R} {\left(\frac {\partial l_\xi(x \mid \theta)} {\partial \theta} \right)}^2 \frac 1 {l_\xi(x \mid \theta)} dx
      = \\ &=
      \frac {\partial^2} {\partial \theta^2} \int_{\R} l_\xi(x \mid \theta) dx - \int_{\R} {\left(\frac {\partial \ln l_\xi(x \mid \theta)} {\partial \theta} \cdot l_\xi(x \mid \theta) \right)}^2 \frac 1 {l_\xi(x \mid \theta)} dx
      = \\ &=
      -\FisherInfo_\xi(\theta).
    \end{align*}
  \end{enumerate}
\end{proof}

\subsection{Ефективни оценки}

\begin{definition}
  Казваме, че неизместената точкова оценка \( t_n \) на числената характеристика \( \theta \) за случайната величина \( \xi \) е \underLine{ефективна}, ако
  \begin{align*}
    \Var(t_n \mid \theta) = \Var(t_n(\xi_1, \ldots, \xi_n) \mid \theta) = \frac 1 {\FisherInfo_\xi(\theta)}.
  \end{align*}

  Разбира се, това изисква за \( l_\xi(x \mid \theta) \) да бъдат изпълнени условията за регулярност.
\end{definition}

\begin{theorem}
  Ако за вектор от числени характеристики \( \theta = (\theta_1, \ldots, \theta_m) \) на случайната величина \( \xi \) съществува неизместена оценка с минимална дисперсия, тя е единствена.
\end{theorem}
\begin{proof}
  Нека \( t_n \) и \( u_n \) са неизместени оценки за \( \theta \) с минимална дисперсия \( d^2 \). Ще докажем, че те са равни.

  Нека \( u_n = (t_n + u_n) / 2 \). Поради линейността на очакването, \( v_n \) също е неизместена оценка на \( \theta \). За дисперсията на \( v_n \) имаме
  \begin{align*}
    \Var(v_n \mid \theta)
    =
    \frac 1 4 \Expect \left( {(t_n + u_n)}^2 \mid \theta \right)
    =
    \frac 1 4 [\Var(t_n \mid \theta) + \Var(u_n \mid \theta) + 2 \Cov(t_n, u_n \mid \theta)].
  \end{align*}

  Неравенството на Коши-Буняковски-Шварц ни дава
  \begin{align*}
    \Cov(t_n, u_n \mid \theta)
    =
    \Expect(t_n u_n \mid \theta)
    \leq
    \sqrt {\Expect(t_n^2 \mid \theta) \Expect(u_n^2 \mid \theta)}
    =
    d^2.
  \end{align*}

  Така получихме \( \Var(v_n \mid \theta) \leq d^2 \). Тъй като дисперсията \( d^2 \) е минимална, строгото неравенство \( \Var(v_n \mid \theta) < d^2 \) няма как да е изпълнено. Остава \( \Var(v_n \mid \theta) = \Cov(t_n, u_n \mid \theta) = d^2 \) и значи в горното неравенство се достига равенство.

  Но равенство в неравенството на Коши-Буняковски-Шварц може да се достигне единствено когато \( t_n \) и \( u_n \) са линейно зависими, т.е. \( t_n = \alpha u_n \) за някое \( \alpha \in \R \setminus \{ 0 \} \). Тогава
  \begin{align*}
    d^2 = \Var(t_n \mid \theta) = \alpha^2 \Var(u_n \mid \theta) = d^2,
  \end{align*}
  следователно \( \alpha = 1 \) и \( t_n = u_n \).
\end{proof}

\begin{theorem}[Рао-Крамер]
  Ефективните оценки имат минимална дисперсия.
\end{theorem}

Ще докажем малко по-общ резултат, който улеснява проверката и даже намирането на ефективни оценки.
\begin{theorem}[Рао-Крамер]\label{thm:rao-cramer}
  Нека \( \theta \) е някаква числена характеристика на \( \xi \) е \( r'(\theta) \) е диференцируема в \( \Theta \). Нека са изпълнени условията за регулярност от деф.~\ref{def:fisher-information}.

  За всяка неизместена оценка \( t_n \) на \( r(\theta) \) е изпълнено
  \begin{align*}
    \Var(t_n \mid \theta) \geq \frac {{\left[r'(\theta) \right]}^2} {n \FisherInfo_\xi(\theta)},
  \end{align*}
  като равенство се достига тогава и само тогава, когато производната на логаритмичното правдоподобие на извадката \( \xi_1, \ldots, \xi_n \) допуска представяне
  \begin{align*}
    \frac {\partial \ln l(x_1, \ldots, x_n \mid \theta)} {\partial \theta}
    =
    k(\theta) [t_n(x_1, \ldots, x_n) + r(\theta)],
  \end{align*}
  където \( k: \Theta \to \R \) не зависи от \( x_1, \ldots, x_n \).
\end{theorem}
\begin{proof}
  Отново ще докажем теоремата само за абсолютно непрекъснати разпределения. В общия случай римановите интеграли могат да се заменят с интеграли по вероятностни мерки, съответстващи на различните параметри \( \theta \in \Theta \).

  Диференцираме интегралите
  \begin{align*}
     1 &= \int_{\R^n} l(x_1, \ldots, x_n \mid \theta) dx_1 \ldots dx_n,
     \\
     \Expect(t_n \mid \theta) &= \int_{\R^n} l(x_1, \ldots, x_n \mid \theta) t(x_1, \ldots, x_n) dx_1 \ldots dx_n.
  \end{align*}
  по \( \theta \)
  \begin{align*}
     0 &= \int_{\R^n} \frac {\partial l(x_1, \ldots, x_n \mid \theta)} {\partial \theta} dx_1 \ldots dx_n,
     \\
     r'(\theta) &= \int_{\R^n} \frac {\partial l(x_1, \ldots, x_n \mid \theta)} {\partial \theta} t(x_1, \ldots, x_n) dx_1 \ldots dx_n.
  \end{align*}

  Тогава
  \begin{align*}
    r'(\theta)
    &=
    r'(\theta) - 0 \cdot r(\theta)
    = \\ &=
    \int_{\R^n} \frac {\partial l(x_1, \ldots, x_n \mid \theta)} {\partial \theta} \cdot [t(x_1, \ldots, x_n) - r(\theta)] dx_1 \ldots dx_n
    = \\ &=
    \int_{\R^n} \frac {\partial \ln l(x_1, \ldots, x_n \mid \theta)} {\partial \theta} \cdot l(x_1, \ldots, x_n) \cdot [t(x_1, \ldots, x_n) - r(\theta)] dx_1 \ldots dx_n
    = \\ &=
    \Expect \left( \frac {\partial \ln l(\xi_1, \ldots, \xi_n \mid \theta)} {\partial \theta} \cdot [t(\xi_1, \ldots, \xi_n) - r(\theta)] \mid \theta \right).
  \end{align*}

  От неравенството на Коши-Буняковски-Шварц получаваме
  \begin{align*}
    {\left(r'(\theta) \right)}^2
    &=
    {\left( \Expect \left( \frac {\partial \ln l(\xi_1, \ldots, \xi_n \mid \theta)} {\partial \theta} \cdot [t(\xi_1, \ldots, \xi_n) - r(\theta)] \mid \theta \right)\right)}^2
    \leq \\ &\leq
    \Expect \left( {\left(\frac {\partial \ln l(\xi_1, \ldots, \xi_n \mid \theta)} {\partial \theta} \right)}^2 \mid \theta \right) \cdot \Expect \left( {\left[t(\xi_1, \ldots, \xi_n) - r(\theta) \right]}^2 \mid \theta \right)
    = \\ &=
    n \FisherInfo_\xi(\theta) \Var \left( t_n \mid \theta \right),
  \end{align*}
  като равенство се достига, когато за някоя зависеща от \( \theta \) константа \( k(\theta) \) е изпълнено
  \begin{align*}
    \frac {\partial \ln l(x_1, \ldots, x_n \mid \theta)} {\partial \theta} = k(\theta) (t(x_1, \ldots, x_n) - r(\theta)).
  \end{align*}
\end{proof}

\subsection{Нормално разпределение}

Нека \( \xi \in \DNormal(\mu, \sigma^2) \) и \( \xi_1, \ldots, \xi_n \) е проста извадка над \( \xi \).

\begin{proposition}\label{thm:normal-mean-variance}
  Оценките \( m_n \) и \( s_n^2 \) за \( \DNormal(\mu, \sigma^2) \)-разпределена извадка са независими. За разпределенията им имаме
  \begin{align*}
    m_n \in \DNormal \left(\mu, \frac {\sigma^2} n \right),
    \\
    (n-1) \frac {s_n^2} {\sigma^2} \in \DChiSq(n-1).
  \end{align*}
\end{proposition}
\begin{proof}
  Този факт следва от теоремата на Кокрън. Наистина, нека \( \eta_k = \frac {\xi_k - \mu} \sigma, k = 1, \ldots, n \).

  Разглеждаме квадратичните форми
  \begin{align*}
    (n-1) s_n^2(\eta_1, \ldots, \eta_n)
    &=
    \sum_{k=1}^n \xi_k^2 - {[\sqrt n \cdot m_n(\eta_1, \ldots, \eta_n)]}^2,
    \\
    \sum_{i=1}^n {\left(\eta_i - \frac 1 n \sum_{j=1}^n \eta_j \right)}^2
    &=
    \sum_{k=1}^n \eta_k^2 - \frac 1 n {\left(\sum_{k=1}^n \eta_k \right)}^2,
    \\
    \V{\eta_n}^T \left(I_n - \frac 1 n 1_n \right) \V{\eta_n}
    &=
    \V{\eta_n}^T I_n \V{\eta_n} - \V{\eta_n}^T \left(\frac 1 n 1_n \right) \V{\eta_n},
  \end{align*}
  където с \( I_n \) сме означили единичната \( n \times n \) матрица и
  \begin{align*}
    1_n
    \coloneqq
    \begin{pmatrix}
      1 & \cdots & 1 \\
      \vdots & \ddots & \vdots \\
      1 & \cdots & 1
    \end{pmatrix}.
  \end{align*}

  Имаме равенство и за ранговете на матриците на квадратичните форми, т.е.
  \begin{align*}
    \left(I_n - \frac 1 n 1_n \right) = \Rank I_n - \Rank \left( \frac 1 n 1_n \right).
  \end{align*}

  От теоремата на Кокрън следва, че
  \begin{align*}
    {[\sqrt n m_n(\eta_1, \ldots, \eta_n)]}^2 &\in \DChiSq(1),
    \\
    (n-1) s_n^2(\eta_1, \ldots, \eta_n) &\in \DChiSq(n - 1)
  \end{align*}
  и освен това двете са независими.

  Изразено чрез оригиналната извадка \( \xi_1, \ldots, \xi_n \), получаваме:
  \begin{enumerate}
    \item Оценката на очакването \( m_n \) е \( \DNormal(\mu, n \sigma^2) \)-разпределена. Наистина,
    \begin{align*}
      {\left(\frac 1 {\sqrt n} \sum_{k=1}^n \frac {\xi_k - \mu} \sigma \right)}^2
      &=
      {\left( \sqrt n \frac {m_n(\xi_1, \ldots, \xi_n) - \mu} \sigma \right)}^2
      = \\ &=
      {[\sqrt n \cdot m_n(\eta_1, \ldots, \eta_n)]}^2
      \in
      \DChiSq(1),
    \end{align*}
    следователно
    \begin{align*}
      \sqrt n \frac {m_n(\xi_1, \ldots, \xi_n) - \mu} \sigma &\in \DNormal(0, 1),
      \\
      m_n(\xi_1, \ldots, \xi_n) &\in \DNormal \left(\mu, \frac {\sigma^2} n \right).
    \end{align*}

    \item Коригираната оценка на дисперсията \( (n-1) \frac {s_n^2} {\sigma^2} \) е \( \DChiSq(n-1) \)-разпределена. Наистина,
    \begin{align*}
      (n-1) \frac {s_n^2(\xi_1, \ldots, \xi_n)} {\sigma^2}
      =
      (n-1) s_n^2(\eta_1, \ldots, \eta_n)
      \in
      \DChiSq(n - 1).
    \end{align*}

    \item Оценките \( m_n \) и \( s_n^2 \) за оригиналната извадка са независими, тъй като съответните оценки за стандартизираната извадка са техни афинни преобразования и са независими.
  \end{enumerate}
\end{proof}

Това твърдение мотивира следната дефиниция
\begin{definition}
  Стандартизация на извадката \( \xi_1, \ldots, \xi_n \) наричаме случайната величина
  \begin{align*}
    \sqrt n \frac {m_n - \mu} \sigma \in \DNormal(0, 1).
  \end{align*}

  Аналогично на стандартизацията, \underLine{стюдентизация} наричаме случайната величина
  \begin{align*}
    \sqrt n \frac {m_n - \mu} {s_n} \in \DStudent(n - 1).
  \end{align*}
\end{definition}

\begin{note}
  Стюдентизацията има разпределение на Стюдънт, тъй като според твърдение~\ref{thm:normal-mean-variance} оценките \( m_n \) и \( s_n \) са независими.
\end{note}

\subsubsection{Доверителен интервал за очакването при известна дисперсия}

Със \( z_p \) ще означаваме \( p \)-квантила на стандартното нормално разпределение, т.е. \( \Phi(z_p) = p \).

Да забележим, че стандартното нормално разпределение е симетрично около нулата, за нас е достатъчно да знаем само един от двата квантила
\begin{align*}
  -z_{\frac {1-\gamma} 2}
  =
  z_{1-\frac {1+\gamma} 2}
  =
  z_{\frac {1+\gamma} 2}.
\end{align*}

Разглеждаме следния \( \gamma \)-доверителен интервал с ниво на доверие
\begin{align*}
  \Prob \left( z_{\frac {1-\gamma} 2} \leq \sqrt n \frac {m_n - \mu} \sigma \leq -z_{\frac {1-\gamma} 2} = z_{\frac {1+\gamma} 2} \right) &= \gamma,
  \\
  \Prob \left( -z_{\frac {1-\gamma} 2} \leq \sqrt n \frac {\mu - m_n} \sigma \leq z_{\frac {1-\gamma} 2} \right) &= \gamma,
  \\
  \Prob \left( m_n - \frac \sigma {\sqrt n} z_{\frac {1-\gamma} 2} \leq \mu \leq m_n + \frac \sigma {\sqrt n} z_{\frac {1-\gamma} 2} \right) &= \gamma.
\end{align*}

\subsubsection{Доверителен интервал за очакването при неизвестна дисперсия}

С \( {\DStudent(m)}_p \) ще означаваме \( p \)-квантила на \( \DStudent(m) \) разпределението на Стюдънт.

Аналогично със случая с известно средно, но използвайки стюдентизация вместо стандартизация, получаваме интервала
\begin{align*}
  \Prob \left( m_n - \frac {s_n} {\sqrt n} \cdot {\DStudent(n-1)}_{\frac {1-\gamma} 2} \leq \mu \leq m_n + \frac {s_n} {\sqrt n} \cdot {\DStudent(n-1)}_{\frac {1-\gamma} 2} \right) = \gamma.
\end{align*}

\subsubsection{Доверителен интервал за стандартното отклонение}

С \( {\DChiSq(m)}_p \) ще означаваме \( p \)-квантила на \( \DChiSq(m) \) разпределението.

Тъй като според твърдение~\ref{thm:normal-mean-variance} имаме \( (n-1) \frac {s_n^2} {\sigma^2} \in \DChiSq(n-1) \), директно намираме \( \gamma \)-доверителния интервал
\begin{align*}
  \Prob \left( {\DChiSq(n-1)}_{\frac {1-\gamma} 2} \leq (n-1) \frac {s_n^2} {\sigma^2} \leq {\DChiSq(n-1)}_{\frac {1+\gamma} 2} \right) &= \gamma,
  \\
  \Prob \left( \frac {{\DChiSq(n-1)}_{\frac {1-\gamma} 2}} {(n-1) s_n^2} \leq \frac 1 {\sigma^2} \leq \frac {{\DChiSq(n-1)}_{\frac {1+\gamma} 2}} {(n-1) s_n^2} \right) &= \gamma,
  \\
  \Prob \left( \frac {(n-1) s_n^2} {{\DChiSq(n-1)}_{\frac {1+\gamma} 2}} \leq \sigma^2 \leq \frac {(n-1) s_n^2} {{\DChiSq(n-1)}_{\frac {1-\gamma} 2}} \right) &= \gamma.
\end{align*}

\printbibliography

\end{document}
